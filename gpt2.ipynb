{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257 , #vocabulary size\n",
    "    \"context_length\" : 256, #context length{orignal = 1024}\n",
    "    \"emb_dim\"  : 768, #output embeddings dimension\n",
    "    \"n_heads\" : 12, #number of attention heads\n",
    "    \"n_layers\" : 12, # number of layers\n",
    "    \"drop_rate\" : 0.1, #dropout rate \n",
    "    \"qkv_bias\" : False #key - Query -value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "starting with tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves youEvery feminists Sadfellipes Being accumulation Burn famousMine\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,  Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer,max_len,stride):\n",
    "        self.text = text\n",
    "        self.tokenizer = tokenizer\n",
    "        idx=tokenizer.encode(text)\n",
    "        self.input_ids=[]\n",
    "        self.target_ids=[]\n",
    "        for i in range(0,len(idx)-max_len,stride):\n",
    "          self.input_ids.append(idx[i:i+max_len])\n",
    "          self.target_ids.append(idx[i+1:i+max_len+1])\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.input_ids[index]),torch.tensor(self.target_ids[index])\n",
    "    def __len__(self):\n",
    "      return len(self.input_ids)\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset =   TextDataset(text=txt, tokenizer=tokenizer, max_len=max_length, stride=stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention mechanism\n",
    "        \n",
    "        Args:\n",
    "        - d_in: Input dimension (embedding size of each token)\n",
    "        - d_out: Output dimension (total features after attention)\n",
    "        - context_length: Number of tokens in a sequence\n",
    "        - dropout: Dropout rate\n",
    "        - num_heads: Number of attention heads\n",
    "        - qkv_bias: Whether to use bias in Q, K, V projections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_out = d_out  # Total output dimension\n",
    "        self.num_heads = num_heads  # Number of heads\n",
    "        self.head_dim = d_out // num_heads  # Dimension per head (each head processes this much info)\n",
    "        \n",
    "        # Linear layers for query, key, and value projections\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "        # Final projection layer after multi-head attention\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Upper triangular mask for causal attention (prevents attending to future tokens)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "        - x: Input tensor of shape (batch_size, num_tokens, d_in)\n",
    "        \n",
    "        Returns:\n",
    "        - context_vec: Output tensor after attention (batch_size, num_tokens, d_out)\n",
    "        \"\"\"\n",
    "        b, num_tokens, d_in = x.shape  # Batch size, number of tokens, input feature size\n",
    "        \n",
    "        # Compute queries, keys, and values\n",
    "        keys = self.W_key(x)  # (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)  # (b, num_tokens, d_out)\n",
    "        values = self.W_value(x)  # (b, num_tokens, d_out)\n",
    "        \n",
    "        # Reshape into multiple heads: (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Rearrange dimensions: (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores using dot product: (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        \n",
    "        # Convert the mask to boolean and match size to (num_tokens, num_tokens)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        \n",
    "        # Apply mask (set masked positions to -inf so softmax gives zero probability)\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        # Compute attention weights using softmax (scaled by sqrt(head_dim) for stability)\n",
    "        attn_weights = torch.softmax(attn_scores / self.head_dim ** 0.5, dim=-1)\n",
    "        \n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Compute the context vector (weighted sum of values): (b, num_heads, num_tokens, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)  # Swap back num_heads and num_tokens\n",
    "        \n",
    "        # Reshape context vector back to (b, num_tokens, d_out)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        \n",
    "        # Apply final output projection layer\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        \n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Layer Normalization: Normalizes inputs to have zero mean and unit variance per feature\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):  # emb_dim: Dimensionality of input embeddings\n",
    "        super().__init__()  # Initialize the parent class (nn.Module)\n",
    "        self.eps = 1e-5  # Small constant to prevent division by zero during normalization\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))  # Learnable scaling factor for normalization\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))  # Learnable shifting factor for normalization\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)  # Compute mean along the last dimension (per feature)\n",
    "        var = x.var(dim=-1, keepdim=True)  # Compute variance along the last dimension (per feature)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)  # Normalize input (zero mean, unit variance)\n",
    "        return self.scale * norm_x + self.shift  # Apply learnable scale and shift\n",
    "\n",
    "# Gaussian Error Linear Unit (GELU): A smoother alternative to ReLU activation function\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Initialize parent class (nn.Module)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # GELU formula approximated using tanh function\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "# Feed Forward Network: Used in Transformer models to project embeddings to higher dimensions and back\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):  # cfg: Dictionary containing model configuration\n",
    "        super().__init__()  # Initialize parent class (nn.Module)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),  # Expand embedding dimension by 4x\n",
    "            GELU(),  # Apply GELU activation function for non-linearity\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])  # Reduce back to original embedding dimension\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)  # Pass input through the sequential feed-forward layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''we will make a transformer block to align things which we defined and\n",
    "studied earlier this part of code requires care and attention because if you do not\n",
    "understand things from theory it will be very hard to place things at right position for this \n",
    "i suggest to use a diagram to code'''\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer block that consists of:\n",
    "    - Multi-Head Self-Attention\n",
    "    - Feed-Forward Network\n",
    "    - Layer Normalization\n",
    "    - Residual Connections\n",
    "    - Dropout\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-Head Self-Attention Layer\n",
    "        # It allows the model to focus on different parts of the input sequence\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],           # Input embedding dimension\n",
    "            d_out=cfg[\"emb_dim\"],          # Output dimension remains the same\n",
    "            context_length=cfg[\"context_length\"],  # Maximum sequence length\n",
    "            num_heads=cfg[\"n_heads\"],      # Number of attention heads\n",
    "            dropout=cfg[\"drop_rate\"],      # Dropout for regularization\n",
    "            qkv_bias=cfg[\"qkv_bias\"]       # Whether to include bias in QKV projections\n",
    "        )\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        # A two-layer MLP to process each token independently after attention\n",
    "        self.ff = FeedForward(cfg)\n",
    "        \n",
    "        # Layer Normalization to stabilize training and normalize activations\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])  # Applied before attention\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])  # Applied before feed-forward network\n",
    "        \n",
    "        # Dropout for preventing overfitting and improving generalization\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"]) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer block with residual connections.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save the original input for the residual (shortcut) connection\n",
    "        shortcut = x  \n",
    "        \n",
    "        # Apply Layer Normalization before Attention (Pre-Norm Transformer)\n",
    "        x = self.norm1(x)  \n",
    "        \n",
    "        # Apply Multi-Head Self-Attention to capture dependencies between tokens\n",
    "        x = self.att(x)  \n",
    "        \n",
    "        # Apply dropout to the attention output for regularization\n",
    "        x = self.drop_shortcut(x)  \n",
    "        \n",
    "        # Add the residual connection (original input + attention output)\n",
    "        x = x + shortcut  \n",
    "        \n",
    "        # Save the current output as a new shortcut for the next residual connection\n",
    "        shortcut = x  \n",
    "        \n",
    "        # Apply Layer Normalization before the Feed-Forward Network\n",
    "        x = self.norm2(x)  \n",
    "        \n",
    "        # Apply the Feed-Forward Network (MLP) to process each token independently\n",
    "        x = self.ff(x)  \n",
    "        \n",
    "        # Apply dropout to the feed-forward output\n",
    "        x = self.drop_shortcut(x)  \n",
    "        \n",
    "        # Add the residual connection (original input + FFN output)\n",
    "        x = x + shortcut  \n",
    "        \n",
    "        return x  # Return the final transformed output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    \"\"\" \n",
    "    GPT Model: Implements a simplified GPT-style transformer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"\n",
    "        Initializes the GPT model.\n",
    "        \n",
    "        Args:\n",
    "            cfg (dict): Configuration dictionary containing:\n",
    "                - vocab_size (int): Number of unique tokens in vocabulary.\n",
    "                - emb_dim (int): Dimensionality of token embeddings.\n",
    "                - context_length (int): Maximum sequence length.\n",
    "                - drop_rate (float): Dropout probability.\n",
    "                - n_layers (int): Number of transformer blocks.\n",
    "                - n_heads (int): Number of attention heads per transformer block.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Token Embedding: Converts token indices to dense vectors of size `emb_dim`\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "\n",
    "        # Positional Embedding: Provides position information to each token\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "\n",
    "        # Dropout layer for embeddings to prevent overfitting\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # Transformer Blocks: Stack of multiple Transformer layers\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        # Final Layer Normalization to stabilize outputs before prediction\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "\n",
    "        # Output Linear Layer: Maps final embeddings to vocabulary logits for next-token prediction\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        \"\"\"\n",
    "        Forward pass through the GPT model.\n",
    "        \n",
    "        Args:\n",
    "            in_idx (Tensor): Input tensor of shape (batch_size, seq_length) with token indices.\n",
    "\n",
    "        Returns:\n",
    "            logits (Tensor): Output tensor of shape (batch_size, seq_length, vocab_size).\n",
    "        \"\"\"\n",
    "        # Extract batch size and sequence length from input shape\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "\n",
    "        # Token embeddings: Converts token indices into dense vector representations\n",
    "        tok_embeddings = self.tok_emb(in_idx)\n",
    "\n",
    "        # Positional embeddings: Create a sequence of position indices and get embeddings\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "\n",
    "        # Sum token embeddings and positional embeddings (element-wise)\n",
    "        x = tok_embeddings + pos_embeds\n",
    "\n",
    "        # Apply dropout to embeddings\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # Pass through stacked Transformer blocks\n",
    "        x = self.trf_blocks(x)\n",
    "\n",
    "        # Apply final layer normalization\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # Pass through output head to get logits for each token position\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''now coding a function to predict next token based on propability '''\n",
    "\n",
    "def generate_text_simple(model , idx , max_new_tokens , context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        #if LLM supports only 5 tokens and the context size is 10\n",
    "        #then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[: , -context_size:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        #focus only on last time step\n",
    "        logits = logits[: , -1 , :]\n",
    "        \n",
    "        #apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits , dim = -1) #(batch,vocab_size)\n",
    "        \n",
    "        #get the idx of the vocab entry with highest probability value\n",
    "        idx_next = torch.argmax(probas , dim = -1 , keepdim = True)\n",
    "        \n",
    "        idx = torch.cat((idx , idx_next), dim = 1) #(batch , n_tokens+1)\n",
    "        \n",
    "    return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_from_local_file(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "        return text_data\n",
    "    else:\n",
    "        print(f\"File not found at: {file_path}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "file_path = \"D:/ChromeDownloads/the-verdict.txt\"\n",
    "\n",
    "text_data = load_text_from_local_file(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=1,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=1,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cacl_loss_batch(input_batch , target_batch , model , device):\n",
    "     input_batch,target_batch = input_batch.to(device) , target_batch.to(device)\n",
    "     logits = model(input_batch)\n",
    "     loss = torch.nn.functional.cross_entropy(logits.flatten(0,1) , target_batch.flatten())\n",
    "     return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader , model , device , num_batches = None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None :\n",
    "        num_batches = len(data_loader)\n",
    "        \n",
    "    else:\n",
    "        #reduce the number of batches to match the total number of batches in the data loader\n",
    "        #if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches , len(data_loader))\n",
    "    for i , (input_batch , target_batch) in enumerate(data_loader):\n",
    "        if i <num_batches:\n",
    "            loss = cacl_loss_batch(input_batch,target_batch,model,device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model , train_loader , val_loader , device ,eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader , model ,device , num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader , model ,device , num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model,tokenizer,device , start_context):\n",
    "  model.eval()\n",
    "  context_size = model.pos_emb.weight.shape[0]\n",
    "  encoded = text_to_token_ids(start_context,tokenizer).to(device)\n",
    "  with torch.no_grad():\n",
    "    token_ids = generate_text_simple(model=model , idx = encoded , max_new_tokens=50 , context_size= context_size\n",
    "                                     )\n",
    "    decoded_text = token_ids_to_text(token_ids , tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\",\" \")) #compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader , optimizer , device , num_epochs , \n",
    "                       eval_freq , eval_iter , start_context , tokenizer):\n",
    "    #initialize lists to track losses and tokens seen\n",
    "    train_losses , val_losses , track_tokens_seen = [],[],[]\n",
    "    tokens_seen , global_step = 0,-1\n",
    "    \n",
    "    #main training loop \n",
    "    for epoch in range (num_epochs):\n",
    "        model.train() #Set model to Training mode\n",
    "        \n",
    "        for input_batch , target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradient from previous batch iteration\n",
    "            loss = cacl_loss_batch(input_batch , target_batch , model ,device)\n",
    "            loss.backward() #Calculate loss gradients\n",
    "            optimizer.step() # update model weights using loss gradients\n",
    "            tokens_seen+= input_batch.numel() #returns the total number of elements (or tokens)\n",
    "            global_step+= 1\n",
    "            \n",
    "            #evaluation step\n",
    "            if global_step % eval_freq ==0:\n",
    "                train_loss , val_loss = evaluate_model(model , train_loader , val_loader , device , eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep{epoch+1}(Step{global_step:06d}):\"\n",
    "                      f\"Train loss {train_loss:.3f} , val loss {val_loss:.3f}\")\n",
    "                \n",
    "        #print a Sample text after each epoch\n",
    "        generate_and_print_sample(model , tokenizer , device , start_context\n",
    "        )\n",
    "        \n",
    "        \n",
    "    return train_losses , val_losses , track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep1(Step000000):Train loss 10.033 , val loss 10.127\n",
      "Ep1(Step000005):Train loss 8.244 , val loss 8.528\n",
      "Ep1(Step000010):Train loss 7.199 , val loss 7.505\n",
      "Ep1(Step000015):Train loss 6.697 , val loss 6.868\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep2(Step000020):Train loss 6.315 , val loss 6.645\n",
      "Ep2(Step000025):Train loss 6.191 , val loss 6.555\n",
      "Ep2(Step000030):Train loss 5.854 , val loss 6.547\n",
      "Ep2(Step000035):Train loss 5.763 , val loss 6.564\n",
      "Every effort moves youHe laughed\"\"                                              \n",
      "Ep3(Step000040):Train loss 5.381 , val loss 6.443\n",
      "Ep3(Step000045):Train loss 5.574 , val loss 6.674\n",
      "Ep3(Step000050):Train loss 5.342 , val loss 6.501\n",
      "Every effort moves you. the picture.      the, I had the of the of the picture. the his the of the of the picture. the of the his the picture. the picture. the of the of the man of the picture. the\n",
      "Ep4(Step000055):Train loss 5.327 , val loss 6.405\n",
      "Ep4(Step000060):Train loss 5.199 , val loss 6.342\n",
      "Ep4(Step000065):Train loss 4.820 , val loss 6.380\n",
      "Ep4(Step000070):Train loss 4.313 , val loss 6.403\n",
      "Every effort moves you know; and, and I was, and I was, I was.       \", I was. \"I, and I had been the picture--and it, and, and I was, I was his\n",
      "Ep5(Step000075):Train loss 4.264 , val loss 6.353\n",
      "Ep5(Step000080):Train loss 4.298 , val loss 6.219\n",
      "Ep5(Step000085):Train loss 2.816 , val loss 6.181\n",
      "Every effort moves you know the fact, and pushed one of the fact, and Mrs.  \"Oh, I was, I was to have to see the fact, I had always to see the donkey.     \"Oh, I had the\n",
      "Ep6(Step000090):Train loss 2.965 , val loss 6.161\n",
      "Ep6(Step000095):Train loss 2.777 , val loss 6.198\n",
      "Ep6(Step000100):Train loss 2.722 , val loss 6.235\n",
      "Ep6(Step000105):Train loss 2.062 , val loss 6.234\n",
      "Every effort moves you know,\" was not that I felt able to the fact with a little of a self-confident moustache, I felt to see a smile behind his own Jack himself, as his own sitters.        \n",
      "Ep7(Step000110):Train loss 2.008 , val loss 6.213\n",
      "Ep7(Step000115):Train loss 1.620 , val loss 6.295\n",
      "Ep7(Step000120):Train loss 1.720 , val loss 6.293\n",
      "Ep7(Step000125):Train loss 1.256 , val loss 6.347\n",
      "Every effort moves you know; and my surprise, and he had to the fact with a laugh: \"Yes--she's an awful, I had been to the display of the picture. \"Oh, and as I had been the man of the hour. I\n",
      "Ep8(Step000130):Train loss 0.818 , val loss 6.337\n",
      "Ep8(Step000135):Train loss 0.827 , val loss 6.389\n",
      "Ep8(Step000140):Train loss 0.718 , val loss 6.435\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony.  \"Ah: make yourself comfortable--and here are the cigars you like.\"  He placed them at my elbow and as I turned, and down the room, stopping now\n",
      "Ep9(Step000145):Train loss 0.623 , val loss 6.469\n",
      "Ep9(Step000150):Train loss 0.513 , val loss 6.509\n",
      "Ep9(Step000155):Train loss 0.380 , val loss 6.617\n",
      "Ep9(Step000160):Train loss 0.361 , val loss 6.571\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I saw that, when Stroud laid in the first\n",
      "Ep10(Step000165):Train loss 0.284 , val loss 6.690\n",
      "Ep10(Step000170):Train loss 0.257 , val loss 6.742\n",
      "Ep10(Step000175):Train loss 0.260 , val loss 6.800\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Training completed in 12.75 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, optimizer, device, num_epochs,\n",
    "    5, 5, \"Every effort moves you\", tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
